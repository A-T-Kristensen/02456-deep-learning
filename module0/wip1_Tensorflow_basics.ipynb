{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Fundamentals\n",
    "This Jupyter Notebook contains a very fast introduction to TensorFlow (TF).\n",
    "This introduciton is **VERY** fast, and leaves many topics untouched or under touched, but should give you enough of an introduction to be able to follow the later notebooks.\n",
    "If you want a deeper dive the following are good places to start:\n",
    "\n",
    "#### External resources\n",
    "* [Official getting started material](https://www.tensorflow.org/get_started/) - collection of good tutorials from beginer to very advanced\n",
    "* **[Python API Guides](https://www.tensorflow.org/api_guides/python/array_ops)** - GREAT place to look up TF works! Has some descriptions of how and why to use different parts.\n",
    "* [Documentation](https://www.tensorflow.org/api_docs/python/) - Short and consice descriptions of how everything in TF works.\n",
    "* [LearningTensorFlow.org](http://learningtensorflow.com/getting_started/) - A website dedicated to teaching TF. They have some good tutorials at varying levels.\n",
    "* [Keynote (TensorFlow Dev Summit 2017)](https://www.youtube.com/watch?v=4n1AHvDvVvw&t=33s). 30 min video describing where TensorFlow is right now, and the underlying design principals.\n",
    "* [TensorFlow at DeepMind (TensorFlow Dev Summit 2017)](https://www.youtube.com/watch?v=VdDmhOCw6J0). 20 min video describing how TensorFlow is used at DeepMind, and some of the results they have been able to achieve with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorFlow\n",
    "TensorFlow is a programming system in which you represent computations as graphs.\n",
    "This graph is then compiled to efficient C/C++ code.\n",
    "This added layer of abstraction makes TensorFlow very flexible.\n",
    "TensorFlow can be interfaced using different languages, with Python being the most common, and it can the same code run seamlessly on different types of hardware.\n",
    "\n",
    "\n",
    "![](tf_overview.png)\n",
    "\n",
    "\n",
    "TensorFlow provides multiple APIs. \n",
    "The lowest level API, **TensorFlow Core**, provides you with fine-grained control.\n",
    "Higher level APIs, such as `tf.contrib.learn`, are built on top of TensorFlow Core, are generally faster and easier to use.\n",
    "They help manage data, training, and inference.\n",
    "This guide begins with an introduction to **TensorFlow Core**.\n",
    "Later, in other exercises, we will demonstrate how to use Tensorflow in a way that is closer to how it is used in the real world.\n",
    "\n",
    "**NB**: The some of the API whose names contain `contrib` are still in development, and their interface may change.\n",
    "\n",
    "To use TensorFlow you need to understand how TensorFlow:\n",
    "* Represents computations as graphs.\n",
    "* Executes graphs in the context of Sessions.\n",
    "* Represents data as tensors.\n",
    "* Uses feeds and fetches to get data into and out of arbitrary operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two basic building blocks of TensorFlow are **tensors** and **operations** (called ops for short).\n",
    "* **Tensors**: The edges in the graph\n",
    "    * A Tensor is a typed multi-dimensional array, and are how information flows through the graph.\n",
    "    * Tensors are used for data and parameters\n",
    "* **Ops**: the nodes in the graph \n",
    "    * An op takes zero or more Tensors, performs some computation, and produces zero or more Tensors.\n",
    "\n",
    "A TensorFlow graph is a description of computations. To compute anything, a graph must be launched in a `Session`. A Session places the graph ops onto `Devices`, such as CPUs or GPUs, and provides methods to execute them. These methods return tensors produced by ops as [numpy](http://www.numpy.org/) ndarray objects in Python, and as `tensorflow::Tensor` instances in C and C++.\n",
    "\n",
    "TensorFlow can be used from C, C++, and Python programs, with Python being the most common and best supported.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import TensorFlow, and some other handy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Python 2/3 compatability\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Import libraries\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join('.', '..')) # Allow us to import shared custom \n",
    "                                         # libraries, like utils.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import utils # contain various helper funcitons that aren't \n",
    "             # important to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations\n",
    "\n",
    "Let us begin with a simple example -- 2D linear regression: $y = ax + b$. Where $x$ is the input, $y$ is the output, and $a,~b$ are the parameters.\n",
    "\n",
    "For starters let us compute $y$ when $a=2$, and $b=-1$ for a couple of different $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Building the computational graph\n",
    "\n",
    "# In case we have already created something: clear it\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create the two variables\n",
    "# The 'name' argument indicates what TF should call the variable internally.\n",
    "# It is a good idea to properly name your ops, as it makes debugging and later analysis much easier!\n",
    "a = tf.Variable(2., name=\"a\")\n",
    "b = tf.Variable(-1., name=\"b\")\n",
    "\n",
    "# Create an x variable, with some numbers\n",
    "x_values = [-2, -1, 0, 1, 2]\n",
    "x_var = tf.Variable(x_values, name=\"xVariable\", dtype=tf.float32)\n",
    "\n",
    "# Define y\n",
    "with tf.name_scope('yFromVariable'): # Give y a name\n",
    "    y_from_var = a*x_var + b\n",
    "\n",
    "print(y_from_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What just happened?\n",
    "Why didn't `print(y_from_var)` print out the answer?\n",
    "\n",
    "You might have expected `y_from_var` to be the results of `a*x_var + b`, but instead we got an **Tensor** object.\n",
    "This is because before TensorFlow works by first creating a computational graph representation.\n",
    "This graph can then be compiled, which can then be run.\n",
    "This means that `y_from_var` is a TF **operation**, i.e. a node in the computational graph.\n",
    "\n",
    "Lets compile and run `y_from_var` now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an operation that will initialize the graph.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# TensorFlow operations are performed by 'Sessions'\n",
    "with tf.Session() as sess:\n",
    "    # Run the operation that initializes the graph. \n",
    "    # We almost always do this as the first thing.\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Compute y by running the operation\n",
    "    y_output = sess.run(y_from_var)\n",
    "\n",
    "print('y_output is a ' + str(type(y_output)) + '\\n')\n",
    "\n",
    "# Print the results\n",
    "print('{:4s}  {:4s}'.format('x_var', '  y'))\n",
    "for i in range(len(x_values)):\n",
    "    s = \"{:4.1f} : {:4.1f}\"\n",
    "    print(s.format(x_values[i], y_output[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all well an good, but what if we wanted compute this for another $x$?\n",
    "Right now we have defined `x_var` as a `tf.variable`.\n",
    "This makes changing it cumbersome.\n",
    "A better approach is using `tf.placeholder`, which we will do now.\n",
    "\n",
    "A `placeholder` has 3 important arguments:\n",
    "* **`dtype`** specifying what kind of data we are dealing with. Generally use **tf.float32**, as most GPU's are only optimized for 32 bit floating points\n",
    "* **`shape`** lets TF know the dimensions of the variable. Writing `None` allows us to change the number of dimensions, without having to recompile the graph. This however prevents some optimization, so it should be specified when possible.\n",
    "* **`name`** is what TF will call the placeholder internally. For instance this is the name it will use if the placeholder causes an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create x as a placeholder now!\n",
    "x_ph = tf.placeholder(dtype=tf.float32, shape=[None], name=\"xPlaceholder\")\n",
    "\n",
    "# Define another y, using the placeholder x this time\n",
    "with tf.name_scope('yFromPlaceholder'):\n",
    "    y_from_ph = a*x_ph + b\n",
    "\n",
    "x_new_values = [-0.2, -0.1, 0, 0.1, 0.2]\n",
    "\n",
    "## Compute y\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    feed_dict = {x_ph : x_new_values}\n",
    "    y_output = sess.run(y_from_ph, feed_dict=feed_dict)\n",
    "\n",
    "    \n",
    "# Print the results\n",
    "print('{:4s}  {:4s}'.format(' x_ph', '  y'))\n",
    "for i in range(len(x_values)):\n",
    "    s = \"{:4.1f} : {:4.1f}\"\n",
    "    print(s.format(x_new_values[i], y_output[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What just happened?\n",
    "\n",
    "This time, when we created the graph, we created $x$ as a `tf.placeholder`.\n",
    "This means that `x_ph` simply stands in the place of real data.\n",
    "So when we want to compute `y` for a particular value we simply **feed** that value into the graph, using a `feed_dict`.\n",
    "\n",
    "If we wanted to change the values now, we simply need to feed a new value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These **variables** and **palceholders** can be a little hard to wrap your head around at first.\n",
    "It can therefore be a good idea to read up on these:\n",
    "\n",
    "#### External resources:\n",
    "* **Variables**: \n",
    "    [Guide](https://www.tensorflow.org/programmers_guide/variables),\n",
    "    [Documentation](https://www.tensorflow.org/api_docs/python/tf/Variable)\n",
    "* **Placeholders**: \n",
    "    [Documentation](https://www.tensorflow.org/versions/r0.11/api_docs/python/io_ops/placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the graph with TensorBoard\n",
    "When you execute the cell below you should see a graph that represents the work we have done so far.\n",
    "Normally TensorBoard is opened in a separate browser window, but for now we will show it in-line.\n",
    "\n",
    "Click a node to see its attributes and high level information.\n",
    "**Double click a node to expand it**. Try double clicking on one of the y's.\n",
    "Doing so will show you the operations that are necessary to compute $y$, i.e. a multiplication and an addition.\n",
    "This is especially useful for examining the dimensions of your data as it flows through the graph.\n",
    "\n",
    "The TensorBoard graph visualizer is a great tool for examining your model.\n",
    "It is important to use `tf.name_scope` to dutifully name your variables properly!\n",
    "Otherwise the graph visualizer quickly becomes unwieldy and useless.\n",
    "This takes practice, but it is well worth it.\n",
    "Propper usage of `tf.name_scope` also makes debugging easier, so it is a good habbit to get into.\n",
    "\n",
    "In this example we embed the graph visualizer in Jupyter.\n",
    "The visualizer isn't made for this, and not all features are present.\n",
    "Normally you would access the graph visualizer through **TensorBoard**, as we do at the end of this notebook.\n",
    "If you are interested in how to embed TensorBoard in the notebook see the `../utils.py` file.\n",
    "\n",
    "There are many details that we didn't cover here, but here is a good place to start:\n",
    "\n",
    "#### External resources\n",
    "* **TensorBoard**: \n",
    "    [Graph visualization](https://www.tensorflow.org/get_started/graph_viz), \n",
    "    [Visualizing Learning](https://www.tensorflow.org/get_started/summaries_and_tensorboard), \n",
    "    [Embedding Visualization](https://www.tensorflow.org/get_started/embedding_viz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Launch TensorBoard, and visualize the TF graph\n",
    "tmp_def = utils.rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "utils.show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-assignment**: \n",
    "Try and change the argument in `tf.name_scope` when defining `y_from_var` and `y_from_ph` to `y_variable`, and `y_placeholder`.\n",
    "Then run code visualizing TensorBoard again.\n",
    "* Notice what changed? Can you think of when this kind of thing is smart to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Exercise 1.1: Your first TensorFlow op</span>\n",
    "For the first assignment you must implement Pythagoras' famous equation:\n",
    "$$c = \\sqrt{d^2 + e^2}$$\n",
    "\n",
    "You should create $a$ and $b$ as placeholders, and then compute $c$ for $d = {3, 2, 1}$ and $e = {4,5,6}$.\n",
    "\n",
    "It is **important** that you use TF ops for all the computations on the graph.\n",
    "(e.g. use `tf.square` instead of `np.square`)\n",
    "Otherwise TF can't optimize the code properly, and you risk it becoming VERY slow.\n",
    "You can find the TF math ops that you need [here](https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/basic_math_functions).\n",
    "\n",
    "**NB**: You should not overwrite any of the variable names, as this we need them later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_values = [3,2,1]\n",
    "d_values = [4,5,6]\n",
    "\n",
    "## Your code here!\n",
    "# 1) Define the placeholders and the graph\n",
    "\n",
    "# 2) Start a session, and compute the output\n",
    "c_output = [0, 0, 0] # Use this variable name as your output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out the results, and validate you get the correct values\n",
    "true_values = np.sqrt(np.square(e_values) + np.square(d_values))\n",
    "assingment_1_success = True\n",
    "\n",
    "for i in range(len(c_output)):\n",
    "    assingment_1_success = False if not np.abs(true_values[i] - c_output[i]) < 1e-6 else assingment_1_success\n",
    "    print('Corect value {:4.3f}, your value {:4.3f}. '.format(true_values[i], c_output[i]), end='')\n",
    "    if not assingment_1_success: \n",
    "        print(\"Oops :(\")\n",
    "        print(\"\\nSometihng went wrong, and the output isn't as expected.\\\n",
    "               \\nGo back and have a look, or ask someone for help.\")\n",
    "        break\n",
    "    print('Correct!')\n",
    "\n",
    "    \n",
    "if assingment_1_success:\n",
    "    print('\\nGood job! \\nTake a break, strecht your legs, and then continue onwards!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-assignment**:\n",
    "After having successfully completed the assignment go back and run the code TensorBoard visualization code again.\n",
    "* Did you remember to give your variables and placeholders meaningful names, or does everything look like a mess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Linear Regression\n",
    "\n",
    "Using TF as a substitute for numpy with extra steps is hardly inspiring.\n",
    "Here in part 2 we will extend what we learnt in part 1 and begin introducing techniques relevant for deep learning.\n",
    "We will do this through a linear regression example, but rather than use the ordinary least squares method we will use gradient descent.\n",
    "Gradient descent is the backbone of the backpropagation algorithm, which is used in virtually all deep learning applications.\n",
    "\n",
    "In case you aren't familiar with gradient descent Andrew Ng (one of the big names in AI) explains it very well in [this 10 min video](https://www.youtube.com/watch?v=F6GSRDoB-Cg) in his [machine learning course](https://www.coursera.org/learn/machine-learning) (which is great!).\n",
    "But briefly put: You define a differentiable **loss function** that somehow measures how fare away your model is from the data.\n",
    "Iteratively you then update your model to better fit the data by updating the parameters in the negative direction of the gradients.\n",
    "\n",
    "For real valued data we often use the **mean squared error** (MSE) loss function:\n",
    "\n",
    "$$ loss = \\frac{1}{n} \\sum^n (y_{true} - y_{estimated})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get some data.\n",
    "We will just create some artificial, well behaved data to start with.\n",
    "We create both a **training set** and a **validation set**.\n",
    "The training set is used to tune the model parameters, and the validation set is used to evaluate the model during training.\n",
    "It is common to also have a **test set** which is used for a final evaluation of the model, when the training is complete.\n",
    "We won't bother with the test set for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Creating the data\n",
    "n_train = 30\n",
    "n_valid = 30\n",
    "train_input = np.linspace(-1, 1, n_train)\n",
    "train_target = - train_input + np.random.randn(*train_input.shape) * 0.4\n",
    "\n",
    "valid_input = np.linspace(-1, 1, n_valid)\n",
    "valid_target = - valid_input + np.random.randn(*valid_input.shape) * 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the model from we defined in part 1.\n",
    "Before we begin we will visualize the data and the initial (terrible) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    feed_dict = {x_ph : train_input}\n",
    "    y_output = sess.run(y_from_ph, feed_dict=feed_dict)\n",
    "\n",
    "plt.plot(train_input, y_output, c='k', label='Model')\n",
    "plt.scatter(train_input, train_target, c='b', alpha=0.5, label='Training set')\n",
    "plt.scatter(valid_input, valid_target, c='r', alpha=0.5, label='Validation set')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have that out of the way, we can train our model!\n",
    "\n",
    "First we define the loss function, and the TF ops that will update out weights $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now also need a placeholder for y\n",
    "y_ph = tf.placeholder(dtype=tf.float32, shape=[None], name='yPlaceholder')\n",
    "\n",
    "# Define the loss function\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.square(y_ph - y_from_ph))\n",
    "\n",
    "# Define that we wish to use gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# And finally create the op that updates our weights a and b\n",
    "# One of the (many) cool things about TF is that it automatically \n",
    "# differentiates the loss function.\n",
    "train_op = optimizer.minimize(loss, var_list=[a, b])\n",
    "\n",
    "# We can use TensorBoard to track the training progress. \n",
    "# In this case we wish to track the loss\n",
    "summary_loss = tf.summary.scalar(\"performance/loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define where we want to save the TensorBoard summaries\n",
    "timestr = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "logdir = os.path.join('.', 'logdir', timestr)\n",
    "\n",
    "train_dict={x_ph: train_input, y_ph: train_target}\n",
    "valid_dict={x_ph: valid_input, y_ph: valid_target}\n",
    "\n",
    "max_epoch = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # We use two summary writers. This is a hack that allows us to write to two\n",
    "    # different folders, and thus show the graphs in the same plot.\n",
    "    # Which TensorBoard doesn't otherwise allow\n",
    "    summary_writer_train = tf.summary.FileWriter(os.path.join(logdir, 'train'), sess.graph)\n",
    "    summary_writer_valid = tf.summary.FileWriter(os.path.join(logdir, 'valid'), sess.graph)\n",
    "\n",
    "    for e in range(max_epoch):\n",
    "        # Update the parameters, and compute the summary\n",
    "        cost_train, summary_train, _ = sess.run([loss, summary_loss, train_op], feed_dict=train_dict)\n",
    "        # Note that we don't use the train_op on the validation set!\n",
    "        cost_valid, summary_valid = sess.run([loss, summary_loss], feed_dict=valid_dict)\n",
    "\n",
    "        # Write the summaries\n",
    "        summary_writer_train.add_summary(summary_train, e)\n",
    "        summary_writer_valid.add_summary(summary_valid, e)\n",
    "\n",
    "        ## Visualize the training montage!\n",
    "        y_output = sess.run(y_from_ph, {x_ph: train_input})  \n",
    "        plt.plot(train_input, y_output, c='k', label='Model')\n",
    "        plt.scatter(train_input, train_target, c='b', alpha=0.5, label='Training set')\n",
    "        plt.scatter(valid_input, valid_target, c='r', alpha=0.5, label='Validation set')\n",
    "        plt.legend(loc=1)\n",
    "        plt.title('epoch = {:2}. train error = {:4.2}. valid error = {:4.2}'.format(e, cost_train, cost_valid) )\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-assignment**: The training error will generally be worse than the validation error.\n",
    "* Why is that? \n",
    "\n",
    "*(Don't worry if you can't figure it out, we will cover this later)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the loss changes as a function of epochs by opening up TensorBoard.\n",
    "This plot is called the **learning curves**.\n",
    "\n",
    "Open TensorBoard by, a terminal, typing:\n",
    "\n",
    "    tensorboard logdir==<path/to/TensorBoard/logs>\n",
    "\n",
    "In this case `<path/to/TensorBoard/logs>` is simply `logdir`, if you are already in the correct folder.\n",
    "Accessed TensorBoard by opening your browser and entering the URL `localhost:6006`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curves can tell you a lot about how the network is doing, and how well the training is going\n",
    "Have a look at the learning curves.\n",
    "In this case they look close to ideal, that is:\n",
    "* Training and validation loss follow each other closely\n",
    "* The loss asymptotes to a low value\n",
    "\n",
    "In many cases the learning curves won't be this nice, and later we will go into more depth about what can be read from them.\n",
    "\n",
    "While in TensorBoard have a look at the **graphs** tab, and look at how it changed since the first time we looked at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Credits\n",
    "Created by Toke Faurby ([faur](https://github.com/Faur)).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
