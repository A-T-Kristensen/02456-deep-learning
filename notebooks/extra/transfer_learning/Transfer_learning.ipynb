{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "import inception_preprocessing\n",
    "from inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "import os\n",
    "import time\n",
    "import scipy.io\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np \n",
    "from dataset import generate_cats_and_dogs\n",
    "from scipy.misc import imread\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download  and divide the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems the dataset has already been generated (remove 'train' and 'test' folders to generate it again)\n"
     ]
    }
   ],
   "source": [
    "generate_cats_and_dogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.FailedPreconditionError'>, Attempting to use uninitialized value matching_filenames\n",
      "\t [[Node: matching_filenames/read = Identity[T=DT_STRING, _class=[\"loc:@matching_filenames\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](matching_filenames)]]\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "FIFOQueue '_0_input_producer' is closed and has insufficient elements (requested 1, current size 0)\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](WholeFileReaderV2, input_producer)]]\n\nCaused by op 'ReaderReadV2', defined at:\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-6bed9496cc89>\", line 4, in <module>\n    key, image_file = image_reader.read(filename_queue)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/io_ops.py\", line 193, in read\n    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 400, in _reader_read_v2\n    queue_handle=queue_handle, name=name)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): FIFOQueue '_0_input_producer' is closed and has insufficient elements (requested 1, current size 0)\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](WholeFileReaderV2, input_producer)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: FIFOQueue '_0_input_producer' is closed and has insufficient elements (requested 1, current size 0)\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](WholeFileReaderV2, input_producer)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6bed9496cc89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Get an image tensor and print its value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mkey_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: FIFOQueue '_0_input_producer' is closed and has insufficient elements (requested 1, current size 0)\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](WholeFileReaderV2, input_producer)]]\n\nCaused by op 'ReaderReadV2', defined at:\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-6bed9496cc89>\", line 4, in <module>\n    key, image_file = image_reader.read(filename_queue)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/io_ops.py\", line 193, in read\n    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 400, in _reader_read_v2\n    queue_handle=queue_handle, name=name)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Vilstrup/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): FIFOQueue '_0_input_producer' is closed and has insufficient elements (requested 1, current size 0)\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](WholeFileReaderV2, input_producer)]]\n"
     ]
    }
   ],
   "source": [
    "filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(\"train/*/*.jpg\"))\n",
    "\n",
    "image_reader = tf.WholeFileReader()\n",
    "key, image_file = image_reader.read(filename_queue)\n",
    "S = tf.string_split([key],'/')\n",
    "length = tf.cast(S.dense_shape[1],tf.int32)\n",
    "# adjust constant value corresponding to your paths if you face issues. It should work for above format.\n",
    "label = S.values[length - tf.constant(2, dtype=tf.int32)]\n",
    "label = tf.string_to_number(label,out_type=tf.int32)\n",
    "image = tf.image.decode_image(image_file, channels=3) \n",
    "\n",
    "image_size = 299\n",
    "height, width, is_training = (image_size, image_size, True)\n",
    "\n",
    "#Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "image = inception_preprocessing.preprocess_image(image, height, width, is_training)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Required to get the filename matching to run.\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # Coordinate the loading of image files.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(20):\n",
    "        # Get an image tensor and print its value.\n",
    "        key_val, label_val, image_tensor = sess.run([key,label,image])\n",
    "        print(image_tensor.shape)\n",
    "        print(label_val)\n",
    "        print(key_val)\n",
    "\n",
    "\n",
    "    # Finish off the filename queue coordinator.\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_path ='model'\n",
    "checkpoint_name = 'inception_resnet_v2.ckpt'\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "    !curl -O 'http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz'\n",
    "    !tar -xf 'inception_resnet_v2_2016_08_30.tar.gz'\n",
    "    os.rename('inception_resnet_v2_2016_08_30.ckpt', '{}/{}'.format(model_path, checkpoint_name))\n",
    "    os.remove('inception_resnet_v2_2016_08_30.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#State where your checkpoint file is\n",
    "checkpoint_file = '{}/{}'.format(model_path, checkpoint_name)\n",
    "\n",
    "#State where your log file is at. If it doesn't exist, create it.\n",
    "log_dir = './log'\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "#================= TRAINING INFORMATION ==================\n",
    "#State the number of epochs to train\n",
    "num_epochs = 1\n",
    "\n",
    "#State the number of classes to predict:\n",
    "num_classes = 2\n",
    "num_samples = len(glob('train/*/*.jpg'))\n",
    "\n",
    "#Learning rate information and configuration (Up to you to experiment)\n",
    "initial_learning_rate = 0.0002\n",
    "learning_rate_decay_factor = 0.7\n",
    "num_epochs_before_decay = 2\n",
    "\n",
    "#State your batch size\n",
    "batch_size = 8\n",
    "image_size = 299\n",
    "height=image_size \n",
    "width=image_size \n",
    "is_training=True\n",
    "preprocess_treads=2\n",
    "min_queue_size=256\n",
    "\n",
    "#======================= TRAINING PROCESS =========================\n",
    "#Now we start to construct the graph and build our model\n",
    "with tf.Graph().as_default() as graph:\n",
    "    tf.logging.set_verbosity(tf.logging.INFO) #Set the verbosity to INFO level\n",
    "\n",
    "    # Make a queue of file names including all the JPEG images files in the relative\n",
    "    # image directory.\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        tf.train.match_filenames_once(\"{}/*/*.jpg\".format('train')))\n",
    "\n",
    "\n",
    "    # Read an entire image file which is required since they're JPEGs, if the images\n",
    "    # are too large they could be split in advance to smaller files or use the Fixed\n",
    "    # reader to split up the file.\n",
    "    image_reader = tf.WholeFileReader()\n",
    "\n",
    "    # Read a whole file from the queue, the first returned value in the tuple is the\n",
    "    # filename which we are ignoring.\n",
    "    file_name, image_file = image_reader.read(filename_queue)\n",
    "\n",
    "    # Retrive the correct label from the \n",
    "    S = tf.string_split([file_name],'/')\n",
    "\n",
    "    path_length = tf.cast(S.dense_shape[1],tf.int32)\n",
    "    # Get the parent directory, this is the second last folder in the path\n",
    "    label = S.values[path_length - tf.constant(2, dtype=tf.int32)]\n",
    "    label = tf.string_to_number(label,out_type=tf.int32) # Convert the label into an int\n",
    "\n",
    "    # Decode the image as a JPEG file, this will turn it into a Tensor which we can\n",
    "    # then use in training.\n",
    "    image = tf.image.decode_jpeg(image_file)\n",
    "\n",
    "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "    image = inception_preprocessing.preprocess_image(image, height, width, is_training)\n",
    "\n",
    "    # Take each image into a mini-batch\n",
    "    images, labels = tf.train.shuffle_batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=preprocess_treads,\n",
    "        capacity=min_queue_size + 3 * batch_size,\n",
    "        min_after_dequeue=min_queue_size)\n",
    "    \n",
    "    #Know the number steps to take before decaying the learning rate and batches per epoch\n",
    "    num_batches_per_epoch = int(num_samples / batch_size)\n",
    "    num_steps_per_epoch = num_batches_per_epoch #Because one step is one batch processed\n",
    "    decay_steps = int(num_epochs_before_decay * num_steps_per_epoch)\n",
    "\n",
    "    #Create the model inference\n",
    "    with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "        logits, end_points = inception_resnet_v2(images, num_classes=num_classes, is_training = True)\n",
    "\n",
    "    #Define the scopes that you want to exclude for restoration\n",
    "    exclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']\n",
    "    variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n",
    "\n",
    "    #Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)\n",
    "    one_hot_labels = slim.one_hot_encoding(labels, num_classes)\n",
    "    \n",
    "    #Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n",
    "    total_loss = tf.losses.get_total_loss()    #obtain the regularization losses as well\n",
    "\n",
    "    #Create the global step for monitoring the learning_rate and training.\n",
    "    global_step = get_or_create_global_step()\n",
    "\n",
    "    #Define your exponentially decaying learning rate\n",
    "    lr = tf.train.exponential_decay(\n",
    "        learning_rate = initial_learning_rate,\n",
    "        global_step = global_step,\n",
    "        decay_steps = decay_steps,\n",
    "        decay_rate = learning_rate_decay_factor,\n",
    "        staircase = True)\n",
    "\n",
    "    #Now we can define the optimizer that takes on the learning rate\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "\n",
    "    #Create the train_op.\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "    #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n",
    "    predictions = tf.argmax(end_points['Predictions'], 1)\n",
    "    probabilities = end_points['Predictions']\n",
    "    accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "    metrics_op = tf.group(accuracy_update, probabilities)\n",
    "\n",
    "\n",
    "    #Now finally create all the summaries you need to monitor and group them into one summary op.\n",
    "    tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    my_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    #Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\n",
    "    def train_step(sess, train_op, global_step):\n",
    "        '''\n",
    "        Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\n",
    "        '''\n",
    "        #Check the time for each sess run\n",
    "        start_time = time.time()\n",
    "        total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n",
    "        time_elapsed = time.time() - start_time\n",
    "\n",
    "        #Run the logging to print some results\n",
    "        logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n",
    "\n",
    "        return total_loss, global_step_count\n",
    "\n",
    "    #Now we create a saver function that actually restores the variables from a checkpoint file in a sess\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    def restore_fn(sess):\n",
    "        return saver.restore(sess, checkpoint_file)\n",
    "\n",
    "    #Define your supervisor for running a managed session. Do not run the summary_op automatically or else it will consume too much memory\n",
    "    sv = tf.train.Supervisor(logdir = log_dir, summary_op = None, init_fn = restore_fn)\n",
    "\n",
    "\n",
    "    #Run the managed session\n",
    "    with sv.managed_session() as sess:\n",
    "        for step in range(num_steps_per_epoch * num_epochs):\n",
    "        # for step in xrange(1):\n",
    "            #At the start of every epoch, show the vital information:\n",
    "            if step % num_batches_per_epoch == 0:\n",
    "                logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, num_epochs)\n",
    "                learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n",
    "                logging.info('Current Learning Rate: %s', learning_rate_value)\n",
    "                logging.info('Current Streaming Accuracy: %s', accuracy_value)\n",
    "\n",
    "                # optionally, print your logits and predictions for a sanity check that things are going fine.\n",
    "                logits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\n",
    "                print('logits: \\n', logits_value)\n",
    "                print('Probabilities: \\n', probabilities_value)\n",
    "                print('predictions: \\n', predictions_value)\n",
    "                print('Labels:\\n:', labels_value)\n",
    "\n",
    "            #Log the summaries every 10 step.\n",
    "            if step % 10 == 0:\n",
    "                loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "                summaries = sess.run(my_summary_op)\n",
    "                sv.summary_computed(sess, summaries)\n",
    "\n",
    "            #If not, simply run the training step\n",
    "            else:\n",
    "                loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "\n",
    "        #We log the final training loss and accuracy\n",
    "        logging.info('Final Loss: %s', loss)\n",
    "        logging.info('Final Accuracy: %s', sess.run(accuracy))\n",
    "\n",
    "        #Once all the training has been done, save the log files and checkpoint model\n",
    "        logging.info('Finished training! Saving model to disk now.')\n",
    "        # saver.save(sess, \"./flowers_model.ckpt\")\n",
    "        sv.saver.save(sess, sv.save_path, global_step = sv.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3]",
   "language": "python",
   "name": "Python [py3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
