{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ballpark estimates of hyperparameters\n",
    "__Optimizers:__\n",
    "    1. SGD + Momentum: learning rate 1.0 - 0.1 \n",
    "    2. ADAM: learning rate 3*1e-4 - 1e-5\n",
    "    3. RMSPROP: somewhere between SGD and ADAM\n",
    "\n",
    "__Regularization:__\n",
    "    1. Dropout. Dropout rate 0.1-0.5\n",
    "    2. L2 and L1 regularization - https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.layers.html#regularizers.\n",
    "    Not used that often in deep learning, but 1e-4  -  1e-8.\n",
    "    3. Batchnorm: Batchnorm also acts as a regularizer - https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.layers.html#batch_norm\n",
    "    Often very useful (faster and better convergence)\n",
    "    \n",
    "    \n",
    "__Parameter initialization__\n",
    "    Parameter initialization is extremely important. TensorFlow has a lot of different initializers, check the TensorFlow API [documentation](https://www.tensorflow.org/versions/r0.10/api_docs/index.html). Often used initializer are\n",
    "    1. He - (not available in TensorFlow's API)\n",
    "    2. Glorot - https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.layers.html#xavier_initializer\n",
    "    3. Uniform or Normal with small scale. (0.1 - 0.01) - https://www.tensorflow.org/versions/r0.10/api_docs/python/state_ops.html#random_normal_initializer\n",
    "    4. Orthogonal (I find that this works very well for RNNs) - (not available in TensorFlow's API)\n",
    "\n",
    "Bias is nearly always initialized to zero using the [tf.constant_initializer](https://www.tensorflow.org/versions/r0.10/api_docs/python/state_ops.html#constant_initializer).\n",
    "\n",
    "__Number of hidden units and network structure__\n",
    "   Probably as big network as possible and then apply regularization. You'll have to experiment :). One rarely goes below 512 units for feedforward networks unless your are training on CPU...\n",
    "   Theres is some research into stochstic depth networks: https://arxiv.org/pdf/1603.09382v2.pdf, but in general this is trail and error.\n",
    "\n",
    "__Nonlinearity__: [The most commonly used nonliearities are](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#activation-functions)\n",
    "    \n",
    "    1. ReLU\n",
    "    2. Leaky ReLU. Same as \n",
    "    3. Elu\n",
    "    3. Sigmoids are used if your output is binary. It is not used in the hidden layers. Squases the output between -1 and 1\n",
    "    4. Softmax used as output if you have a classification problem. Normalizes the the output to 1. )\n",
    "\n",
    "\n",
    "See the plot below.\n",
    "\n",
    "__mini-batch size__\n",
    "   Usually people use 16-256. Bigger is not allways better. With smaller mini-batch size you get more updates and your model might converge faster. Also small batchsizes uses less memory  -> you can train a model with more parameters.\n",
    "\n",
    "Hyperparameters can be found by experience (guessing) or some search procedure. Random search is easy to implement and performs decent: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf . \n",
    "More advanced search procedures include [SPEARMINT](https://github.com/JasperSnoek/spearmint) and many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
