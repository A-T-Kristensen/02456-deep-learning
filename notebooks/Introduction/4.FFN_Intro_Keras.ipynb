{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Networks with Keras\n",
    "> <span style=\"color:gray\"> Created by Mikkel Vilstrup ([mvil](https://github.com/MVilstrup))\n",
    "\n",
    "As we try to solve more complex problems with neural networks, the networks' complexity grows in an equal manner.\n",
    "To handle this complexity various researchers and companies have gone in different directions with their frameworks.\n",
    "Some have made it easier to explicitly state what you want, others have made it easier to quickly iterate over possible network structures.\n",
    "There is no wrong or right here, just different implementations. \n",
    "\n",
    "To showcase how different the implementations are, we have included this notebook to showcase Keras. Keras is the most high-level framework at this point. It makes it really easy to try various configurations quickly. It is built on top of various backends, and can thus be utilized on top of almost all the other frameworks. \n",
    "\n",
    "Although Keras makes it easy to iterate over many standard building blocks, it makes it difficult too try to build new building blocks or experiment with small alterations of the standard building blocks specialized for your particular problem. \n",
    "\n",
    "However, when starting out on a new problem it might be beneficial to experiment with Keras initially and then use another framework in the later phases to do the final optimizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join('.', '..')) # in order to import various .py files\n",
    "import utils # contain various helper funcitons that aren't \n",
    "             # important to understand\n",
    "\n",
    "from intro_utils import ffn_layer\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Now we include Tensorflow as well\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import reset_default_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset\n",
    "MNIST is a dataset that is often used for benchmarking. The MNIST dataset consists of 70,000 images of handwritten digits from 0-9. The dataset is split into a 50,000 images training set, 10,000 images validation set and 10,000 images test set. The images are 28x28 pixels, where each pixel represents a normalised value between 0-255 (0=black and 255=white).\n",
    "\n",
    "We use a feedforward neural network to classify the 28x28 mnist images. ``num_features`` is therefore 28x28=784.\n",
    "That is, we represent each image as a vector. The ordering of the pixels in the vector does not matter, so we could permutate all images using the same permutation and still get the same performance.\n",
    "\n",
    "# Notebook layout\n",
    "In this notebook we will first implement a network in Tensorflow, to show what is needed to try and solve MNIST (although quite poorly) with FFNs in this framework. From here we then implement the same network in Keras, which can be done in less than 10 lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To speed up training we'll only work on a subset of the data\n",
    "data = np.load('mnist.npz')\n",
    "num_classes = 10\n",
    "x_train = data['X_train'][:1000].astype('float32')\n",
    "targets_train = data['y_train'][:1000].astype('int32')\n",
    "\n",
    "x_valid = data['X_valid'][:500].astype('float32')\n",
    "targets_valid = data['y_valid'][:500].astype('int32')\n",
    "\n",
    "x_test = data['X_test'][:500].astype('float32')\n",
    "targets_test = data['y_test'][:500].astype('int32')\n",
    "\n",
    "print(\"\"\"\n",
    "Information on dataset\n",
    "----------------------\n",
    "x_train: (amount: {0}, features: {1}), targets_train: {6}\n",
    "x_valid: (amount: {2}, features: {3}), targets_valid: {7}\n",
    "x_test:  (amount: {4}, features: {5}), targets_test: {8}\n",
    "\"\"\".format(len(x_train), x_train.shape[1], len(x_valid), x_valid.shape[1],\n",
    "           len(x_test), x_test.shape[1], len(targets_train), len(targets_valid), len(targets_test)))\n",
    "\n",
    "#plot a few MNIST examples\n",
    "idx = 0\n",
    "canvas = np.zeros((28*10, 10*28))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_train[idx].reshape((28, 28))\n",
    "        idx += 1\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.axis('off')\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('MNIST handwritten digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Tensorflow network\n",
    "\n",
    "The initial network we build here is very simple. We use the building blocks from the former Notebook and simply create two fully connected layers with a Relu activation function in between. We then finalize with a softmax in order to output a probability distribution over the likelyhood the input belongs to a certain class. \n",
    "\n",
    "We train the network with cross entropy since this is the normal loss function to use when utilizing softmax, and train the network with regular stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resetting the graph ...\n",
    "reset_default_graph()\n",
    "\n",
    "num_features = x_train.shape[1]\n",
    "num_classes = 10\n",
    "num_l1_neurons = 512\n",
    "\n",
    "\"\"\"\n",
    "    ----------------- First we define the model -----------------\n",
    "\"\"\"\n",
    "\n",
    "# Setting up placeholder, this is where your data enters the graph!\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, num_features])\n",
    "\n",
    "\n",
    "# Note that ffn_layer is the exact same as in the previous notebook \n",
    "# except we now calculate the input dimensions from the input layer instead of specifying it manually\n",
    "layer1 = ffn_layer('layer-1', input_placeholder, num_l1_neurons)\n",
    "\n",
    "with tf.name_scope('elu-activation'):\n",
    "    layer1_nonlinear = tf.nn.elu(layer1) # you can try with various activation functions!\n",
    "\n",
    "layer2 = ffn_layer('layer-2', layer1_nonlinear, num_classes)\n",
    "\n",
    "with tf.name_scope('output'):\n",
    "    output = tf.nn.softmax(layer2)\n",
    "\n",
    "\"\"\"\n",
    "    ----------------- Then how to train it -----------------\n",
    "\"\"\"    \n",
    "\n",
    "with tf.name_scope('target'):\n",
    "    # y_ is a placeholder variable taking on the value of the target batch.\n",
    "    target_placeholder = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -tf.reduce_sum(target_placeholder * tf.log(output), reduction_indices=[1])\n",
    "\n",
    "    # averaging over samples\n",
    "    loss_tn = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# L2 regularization\n",
    "#with tf.variable_scope('regularization'):\n",
    "#    reg_scale = 0.0001\n",
    "#    regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "#    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "#    reg_term = sum([regularize(param) for param in params])\n",
    "#    loss_tn += reg_term\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    # defining our optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "    # applying the gradients\n",
    "    train_op = optimizer.minimize(loss_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always it is good practice to inspect the network before running it, in order to see if everything is put correctly together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Launch TensorBoard, and visualize the TF graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tmp_def = utils.rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "utils.show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Tensorflow Network\n",
    "\n",
    "This next block might seem a bit frightening. However, the only reason for its complexity is that we want to keep track of the training and validation performance while we are training the network. To do so we have three seperate loops: One for training the network, one for saving the training performance and one for saving the validation performance. \n",
    "\n",
    "**N.B** It is importance to notice it is only the first loop that is needed for training the network in Tensorflow. The rest is just boilerplate code for showing what is happening behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using confusionmatrix to handle \n",
    "from confusionmatrix import ConfusionMatrix\n",
    "\n",
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "loss = []\n",
    "\n",
    "\n",
    "# We run through the specified amount of epochs and train the network at every step\n",
    "for epoch in range(num_epochs):\n",
    "    # We do the actual training of the neural network\n",
    "    cur_loss = 0\n",
    "    for i in range(num_batches_train):\n",
    "        idx = range(i * batch_size, (i + 1) * batch_size) # get indexes for a single batch\n",
    "        x_batch = x_train[idx]\n",
    "        target_batch = targets_train[idx]\n",
    "        \n",
    "        # We create a dictionary of the format:\n",
    "        # tensorflow graph placeholders <- Input data\n",
    "        # In other words, we feed data into the graph through the placeholders\n",
    "        feed_dict_train = {input_placeholder: x_batch, \n",
    "                           target_placeholder: utils.onehot(target_batch, num_classes)}\n",
    "        \n",
    "        # We define what we would like to get out of the graph\n",
    "        # We specify train_op without using it to get tensorflow to update the parameters\n",
    "        fetches_train = [train_op, loss_tn]\n",
    "        _, batch_loss  = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)\n",
    "        cur_loss += batch_loss\n",
    "    \n",
    "    loss.append(cur_loss / batch_size)\n",
    "    \n",
    "    confusion_valid = ConfusionMatrix(num_classes)\n",
    "    confusion_train = ConfusionMatrix(num_classes)\n",
    "\n",
    "    # We run through all the train data again to see how well the network is doing\n",
    "    for i in range(num_batches_train):\n",
    "        idx = range(i * batch_size, (i + 1) * batch_size)\n",
    "        x_batch = x_train[idx]\n",
    "        targets_batch = targets_train[idx]\n",
    "        \n",
    "        # what to feed our accuracy op\n",
    "        feed_dict_eval_train = {input_placeholder: x_batch, \n",
    "                                target_placeholder: utils.onehot(targets_batch, num_classes)}\n",
    "        \n",
    "        # deciding which parts to fetch\n",
    "        fetches_eval_train = [output]\n",
    "        \n",
    "        # running the validation\n",
    "        res = sess.run(fetches=fetches_eval_train, feed_dict=feed_dict_eval_train)\n",
    "        \n",
    "        # collecting and storing predictions\n",
    "        net_out = res[0]\n",
    "        preds = np.argmax(net_out, axis=-1)\n",
    "        confusion_train.batch_add(targets_batch, preds)\n",
    "\n",
    "    # Finally we validate the performance of the network on the validation data\n",
    "    # !Notice! the network have never trained on this data\n",
    "    confusion_valid = ConfusionMatrix(num_classes)\n",
    "    for i in range(num_batches_valid):\n",
    "        idx = range(i * batch_size, (i + 1) * batch_size)\n",
    "        x_batch = x_valid[idx]\n",
    "        targets_batch = targets_valid[idx]\n",
    "        \n",
    "        # what to feed our accuracy op\n",
    "        feed_dict_eval_train = {input_placeholder: x_batch, \n",
    "                                target_placeholder: utils.onehot(targets_batch, num_classes)}\n",
    "        \n",
    "        # deciding which parts to fetch\n",
    "        fetches_eval_train = [output]\n",
    "        \n",
    "        # running the validation\n",
    "        res = sess.run(fetches=fetches_eval_train, feed_dict=feed_dict_eval_train)\n",
    "        \n",
    "        # collecting and storing predictions\n",
    "        net_out = res[0]\n",
    "        preds = np.argmax(net_out, axis=-1) \n",
    "        confusion_valid.batch_add(targets_batch, preds)\n",
    "    \n",
    "    train_acc_cur = confusion_train.accuracy()\n",
    "    valid_acc_cur = confusion_valid.accuracy()\n",
    "\n",
    "    train_acc += [train_acc_cur]\n",
    "    valid_acc += [valid_acc_cur]\n",
    "    print(\"Epoch %i : Train Loss %e , Train acc %f,  Valid acc %f \" \\\n",
    "    % (epoch+1, loss[-1], train_acc_cur, valid_acc_cur))\n",
    "    \n",
    "    if epoch % (num_epochs - 10) == 0:\n",
    "        clear_output()\n",
    "    \n",
    "    \n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch,train_acc,'r',epoch,valid_acc,'b')\n",
    "plt.legend(['Train Acc','Val Acc'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the network in Keras\n",
    "\n",
    "Now that we have built and trained a network in Tensorflow, we will look at how it can be done in Keras. This is actually very simple, as you will soon see. \n",
    "\n",
    "First of all we will import the necessary modules ad classes from Keras.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We remove the tensorflow graph we created above since Keras relies on the Tensorflow Graph and only one can exist at \n",
    "# a time\n",
    "reset_default_graph()\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is needed to be able to work properly in Jupyter Notebooks with Keras\n",
    "# We tell Keras to remove all the elements in the graph it had previously created\n",
    "session = K.get_session()\n",
    "if model is not None:\n",
    "    model.reset_states()\n",
    "\n",
    "\n",
    "# We use the exact same parameters as out implementation in Tensorflow\n",
    "num_features = x_train.shape[1]\n",
    "num_classes = 10\n",
    "num_l1_neurons = 512\n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "\n",
    "# We define a sequential model which simply stack layers on top of each other sequentially\n",
    "model = Sequential()\n",
    "\n",
    "# First we add a Fully-Connected / Dense layer with the 512 hidden units\n",
    "model.add(Dense(units=num_l1_neurons, input_dim=num_features))\n",
    "\n",
    "# Then we add a Relu activation function\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# then we add a new Dense layer with the size of our 10 classes \n",
    "model.add(Dense(units=num_classes))\n",
    "\n",
    "# Finally we add a softmax activation function in order to transform the output into a propability distribution\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# We then transform the model into a tensorflow graph and use Stochastic Gradient Descent as optimization function\n",
    "# and cross entropy as our loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Keras is just a wrapper around Tensorflow it relies completely on Tensorflows graph (or Theano, CNTK or other low level frameworks). This means we can actually inspect the graph that Keras has created in the behind the scenes. \n",
    "\n",
    "However, as you can see it propably provides more confusion than clarity. There is no notion of naming the various keras layers etc. So there is no way to tell it how to display the variables and operations. However, the goal of the Keras API is to make the structure of the network clear from the code. So we don't really need to look at the graph anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Launch TensorBoard, and visualize the TF graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tmp_def = utils.rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "    utils.show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a defined model we can now easily train it using a single function call. The function takes all our:\n",
    "- training inputs\n",
    "- training targets\n",
    "- number of epochs\n",
    "- the decired batch size\n",
    "\n",
    "It then takes care of dividing the data into proper batches and loading it into the graph. On top of this, all the weights are initialized optimally and the defualt hyperparameters are in general the state of the art. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, \n",
    "          utils.onehot(targets_train, num_classes), \n",
    "          epochs=num_epochs, \n",
    "          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is fitted Keras also makes it easy to evaluate  the performance. Like above all it takes is a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, \n",
    "                                utils.onehot(targets_test, num_classes), \n",
    "                                batch_size=128)\n",
    "print()\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    " 1) Notice the difference between the train accuracy and the validation accuracy and try to minimize this difference\n",
    " \n",
    " 2) Try to experiment with both tensorflow and Keras and see if you can improve the perfomance of the network e.g\n",
    "     - More layers\n",
    "     - Include Regularization\n",
    "     - more / less hidden units\n",
    "     - various loss functions\n",
    "     - alternative optimization functions\n",
    " \n",
    " 3) Try to figure out which framework you like the most, and where you might benefit more from one than the other\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Remarks\n",
    "\n",
    "You have now seen in detail how to create feed forward neural networks. With 4 different ways of describing the same general network architecture, it should be obvious there is no `\"right\"` way to define a neural network. In general the only thing rule of thumb is to not try and implement the networks from scratch. There are numerous frameworks which takes care the fundamental concepts are implemented correctly. In this class we will mostly use Tensorflow and show Keras implementations here and there. However, if these are not of your taste, you can take a look at: \n",
    "- [Pytorch](http://pytorch.org/)\n",
    "- [Theano](http://deeplearning.net/software/theano/)\n",
    "- [CNTK](https://github.com/Microsoft/CNTK)\n",
    "- [Chainer](https://github.com/chainer/chainer)\n",
    "... etc.\n",
    "\n",
    "There is no framework that is `better` than the others. However, each has their particular focus. Keras tries to be the easiest to use and takes the most `High-level` approach to Deep Learning. This might be useful at times, however, at others you might want to have more flexibility than such a High-level framework can provide. \n",
    "\n",
    "As you will see, FFNs are just one of many types of architectures and from here we will take a look at the extremely useful alternative called Convolutional Neural Networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
