{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network with Tensorflow\n",
    "> <span style=\"color:gray\"> Created by Mikkel Vilstrup ([mvil](https://github.com/MVilstrup))\n",
    "\n",
    "Now that we have looked at how we can build a network both with traditional (and slow) software practises and speed up the process with linear algebra. We will dive into how deep learning is done in practice. As you have seen by now, deep learning becomes extremely cumbersome if we have to do it from scratch. On top of that there are a lot of numerical errors that can happen if we are not careful *(If you increase the amount of epochs to be in the hundreds in the network from the basic notebook you will most likely experience such errors)*. \n",
    "\n",
    "In this notebook we will work on how to define neural networks through the Deep Learning (Linear Algebra) Framework Tensorflow. We will not spend much time explaining the fundamentals of Tensorflow since there is a notebook dedicated to just this. Instead we will jump directly to solve the Two-Moons problem with the Tensorflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join('.', '..')) # in order to import various .py files\n",
    "import utils # contain various helper funcitons that aren't \n",
    "             # important to understand\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Now we include Tensorflow as well\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import reset_default_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "\n",
    "Neural networks require a lot of data in order to work properly. You'll thus spend quite a lot of your time on the data when working with neural networks. However, this is not the case with this simple toy problem. \n",
    "\n",
    "It is however important to note that we this time have an extra \"dataset\" which we call **X_val**. In the two former notebooks we simply used X_train and X_test. However, in practice it is impossible to know when to stop a neural network from training without a validation set. The neural network might get to 99% accuracy on the training set while still having almost random performance on the test set. In this case it has overfitted to features in the training set that is unique to this particular partition of the data. \n",
    "\n",
    "In order to overcome this problem we tend to extract a tiny partition of the dataset which we call the validation set. This set is not used for training, but still run through the network while training is in progress. We thus use the accuracy of the validation set to judge when it is appropriate to stop the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = 500\n",
    "val_size = 100\n",
    "test_size = 300\n",
    "\n",
    "X_train, Y_train = make_moons(train_size, noise=0.2)\n",
    "X_val, Y_val = make_moons(test_size, noise=0.2)\n",
    "X_test, Y_test = make_moons(test_size, noise=0.2)\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "num_outputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Network / Model\n",
    "\n",
    "In this notebook we have defined a **logistic regression** model in TensorFlow. Some details of TensorFlow can be a bit confusing, however you'll pick them up when you worked with it for some time. We'll once again work with the simple 2-D and 2-class Two-Moons classification problem where the class decision boundary can be visualized. \n",
    "\n",
    "Initially we show that logistic regression can only separate classes linearly. Adding a Non-linear hidden layer to the algorithm permits nonlinear class separation. \n",
    "\n",
    "The building blocks of TensorFlow are variables and operations, with these we can form computational graphs that form neural networks.\n",
    "\n",
    "The [tf.placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) allows us to feed our input data to the computational graph. We can define constraints with the shape of the placeholder to only take a tensor of a certain shape. Note that it is common to provide ``None`` for the first dimension, which allows us to vary the batch size at runtime.\n",
    "\n",
    "The [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable) allows us to store and update Tensors in our graph. Variables are used to build weights for our neural network. Note that we will use a wrapper called [tf.get_variable](https://www.tensorflow.org/api_docs/python/tf/get_variable) througout this tutorial.\n",
    "\n",
    "The [tf.Operation](https://www.tensorflow.org/api_docs/python/tf/Operation) allows us to perform operations on tensors, resulting in new tensors. Such as when computing the logistic regression which is implemented below:\n",
    "\n",
    "$$y = nonlinearity(xW + b)$$\n",
    "\n",
    "where $x$ is the input tensor, $y$ is the output tensor and $\\{W, b\\}$ are the weights (variable tensors). The weights are initialized with an initializer of our choice (check [tensorflow's API](https://www.tensorflow.org/api_docs/) for more.\n",
    "x has shape ```[batchsize, num_features]```. ```W``` has shape ```[num_features, num_units]``` and b has ```[num_units]```. y has then ```[batch_size, num_units]```.\n",
    "\n",
    "In this first exercise we will use basic TensorFlow functions so that you can learn how to build it from scratch. This will help you later if you want to build your own custom operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resets the graph, needed when initializing weights multiple times, like in this notebook\n",
    "reset_default_graph()\n",
    "\n",
    "# Setting up placeholder, this is where your data enters the graph!\n",
    "with tf.name_scope('input'):\n",
    "    input_placeholder = tf.placeholder(tf.float32, [None, num_features])\n",
    "\n",
    "\"\"\"\n",
    "Here we have defined a simple Fully Connected Layer in Tensorflow.\n",
    "\n",
    "In order for the layer to work, it needs the following parameters:\n",
    "    \n",
    "    @name:              The name of the layer. This is to make it simpler to debug\n",
    "    @inputs:            The former layer in the network (Or the input data if it is the first layer)\n",
    "    @output_dimensions: The size of the layer or in other word the amount of \"neurons\" it should have\n",
    "\"\"\"\n",
    "def ffn_layer(name, inputs, input_dimensions, output_dimensions):\n",
    "    # defining our initializer for our weigths from a normal distribution (mean=0, std=0.1)\n",
    "    weight_initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "    \n",
    "    # Encapsulate the operations in a Tensorflow Scope. Not important \n",
    "    # but a nice feature as you will see later\n",
    "    with tf.variable_scope(name):\n",
    "        # Define the weights of this particular layer and randomly initialize their weights\n",
    "        weights = tf.get_variable('{}-weights'.format(name), \n",
    "                                  [input_dimensions, output_dimensions],\n",
    "                                  initializer = weight_initializer)\n",
    "        \n",
    "        # Define the biases of this layer and initialize their weights to zero\n",
    "        biases = tf.get_variable('{}-biases'.format(name), \n",
    "                                 [output_dimensions],\n",
    "                                 initializer = tf.constant_initializer(0.0))\n",
    "        \n",
    "        # Do a matrix multiplication on the inputs and the weights and add the biases to the result\n",
    "        return tf.matmul(inputs, weights) + biases\n",
    "\n",
    "# Setting up ops, these ops will define edges along our computational graph\n",
    "# The below ops will compute a logistic regression, but can be modified to compute\n",
    "# a neural network\n",
    "l_1 = ffn_layer('layer-1', input_placeholder, num_features, num_outputs)\n",
    "\n",
    "# to make a hidden layer we need a nonlinearity\n",
    "#with tf.name_scope('relu-activation'):\n",
    "#    l_1_nonlinear = tf.nn.relu(l_1)\n",
    "\n",
    "with tf.name_scope('output'):\n",
    "    output = tf.nn.softmax(l_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the Network\n",
    "\n",
    "So far our networks is very small, but to do interesting things with neural networks we will need to build much more complicated networks. In as their size and complexity increases it can be a daunting task to debug the networks. To help understand what is going on inside the Tensorflow graph, the Tensorflow team have provided a toolbox they call TensorBoard. A great tool for visualizing the network. \n",
    "\n",
    "Tensorboard allows us to look at the graph and inspect all the operations and variables in detail. This is a great way to fix the most obvious errors as non-connected elements etc. \n",
    "\n",
    "TensorBoard is the reason we envelop our functions in name_scopes. These scopes tells tensorboard how to visualize our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Launch TensorBoard, and visualize the TF graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tmp_def = utils.rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "utils.show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have built the network we have our tensors in our default graph. With this in place we are ready construct the part of the graph used to train the network. To do this we will need to have some targets which we will define as a placeholder just like before. \n",
    "\n",
    "We then need a cost function. There are numerous cost functions to choose from. However, for standard classification you will almost always want to use [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y is a placeholder variable taking on the value of the target batch.\n",
    "with tf.name_scope('targets'):\n",
    "    y = tf.placeholder(tf.float32, [None, num_outputs])\n",
    "\n",
    "with tf.name_scope('cross-entropy'):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -tf.reduce_sum(y * tf.log(output), reduction_indices=[1])\n",
    "\n",
    "    # averaging over samples\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our neural network we need to update the parameters in direction of the negative gradient w.r.t the cost function we defined earlier.\n",
    "We can use `tf.train.Optimizer` to get the gradients (using `compute_gradients`) for all parameters in the network w.r.t ``cost_train``.\n",
    "Imagine that `cost_train` is a function and we want to go downhill. We go downhill by changing the value of the paramters in direction of the negative gradient. \n",
    "\n",
    "Finally we can use the built-in `minimize` to calculate the stochastic gradient descent (SGD) update rule for each paramter in the network.\n",
    "\n",
    "Heres a small animation of gradient descent: http://imgur.com/a/Hqolp . E.g why saddle points might be difficult.\n",
    "To use the other optimizers checkout which optimizers TensorFlow [supports](https://www.tensorflow.org/api_docs/python/tf/train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Optimizer'):\n",
    "    # Defining our optimizer (try with different optimizers here!)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "with tf.name_scope('gradients'):\n",
    "    # Computing our gradients\n",
    "    grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "\n",
    "    # Applying the gradients\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we make the prediction functions, such that we can get an accuracy measure over a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "\n",
    "    # averaging the one-hot encoded vector\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all this in place we have created out final graph. It can be helpful to check once again that everything is as it should be. So we inspect it with TensorBoard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Launch TensorBoard, and visualize the TF graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tmp_def = utils.rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "utils.show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to utilize our `train_op` function repeatedly in order to optimize our weights and biases to make the best possible linear seperation of the half moon dataset.\n",
    "\n",
    "It is worth or read a short introduction on TensorFlow [sessions](https://www.tensorflow.org/api_docs/python/tf/Session) before continuing to the next codeblock. Sessions are used to run TensorFlow graphs, they uses `fetches` to decide which parts of the graph to compute and `feed_dicts` to load data into the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining a function to make predictions using our classifier\n",
    "def pred(X_in, sess):\n",
    "    # first we must define what data to give it\n",
    "    feed_dict = {input_placeholder: X_in}\n",
    "    # secondly our fetches\n",
    "    fetches = [output]\n",
    "    # utilizing the given session (ref. sess) to compute results\n",
    "    res = sess.run(fetches, feed_dict)\n",
    "    # res is a list with each indices representing the corresponding element in fetches\n",
    "    return res[0]\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "\n",
    "train_cost, val_cost, val_acc = [],[],[]\n",
    "\n",
    "# restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    \n",
    "    # initializing all variables \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    utils.plot_decision_boundary(lambda x: pred(x, sess), X_val, Y_val)\n",
    "    plt.title(\"Untrained Classifier\")\n",
    "    \n",
    "    # We iterate over the amount of epochs specified (1 epoch = all train data)\n",
    "    for e in range(num_epochs):\n",
    "        ### TRAINING ###\n",
    "        # what to feed to our train_op\n",
    "        # notice we onehot encode our predictions to change shape from (batch,) -> (batch, num_output)\n",
    "        feed_dict_train = {input_placeholder: X_train, \n",
    "                           y: utils.onehot(Y_train, num_outputs)}\n",
    "        \n",
    "        # deciding which parts to fetch, train_op makes the classifier \"train\"\n",
    "        fetches_train = [train_op, cross_entropy]\n",
    "        \n",
    "        # running the train_op\n",
    "        res = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)\n",
    "        # storing cross entropy (second fetch argument, so index=1)\n",
    "        train_cost += [res[1]]\n",
    "    \n",
    "        ### VALIDATING ###\n",
    "        # what to feed our accuracy op\n",
    "        feed_dict_valid = {input_placeholder: X_val, y: utils.onehot(Y_val, num_outputs)}\n",
    "\n",
    "        # deciding which parts to fetch\n",
    "        fetches_valid = [cross_entropy, accuracy]\n",
    "\n",
    "        # running the validation\n",
    "        res = sess.run(fetches=fetches_valid, feed_dict=feed_dict_valid)\n",
    "        val_cost += [res[0]]\n",
    "        val_acc += [res[1]]\n",
    "\n",
    "        if e % 100 == 0:\n",
    "            print(\"Epoch %i, Train Cost: %0.3f\\tVal Cost: %0.3f\\t Val acc: %0.3f\" \\\n",
    "                  %(e, train_cost[-1],val_cost[-1],val_acc[-1]))\n",
    "        \n",
    "        if e % (num_epochs - 200) == 0: \n",
    "            clear_output()\n",
    "\n",
    "    ### TESTING ###\n",
    "    # what to feed our accuracy op\n",
    "    feed_dict_test = {input_placeholder: X_test, y: utils.onehot(Y_test, num_outputs)}\n",
    "\n",
    "    # deciding which parts to fetch\n",
    "    fetches_test = [cross_entropy, accuracy]\n",
    "\n",
    "    # running the validation\n",
    "    res = sess.run(fetches=fetches_test, feed_dict=feed_dict_test)\n",
    "    test_cost = res[0]\n",
    "    test_acc = res[1]\n",
    "    \n",
    "    print(\"\\nTest Cost: %0.3f\\tTest Accuracy: %0.3f\"%(test_cost, test_acc))\n",
    "    \n",
    "    # For plotting purposes\n",
    "    utils.plot_decision_boundary(lambda x: pred(x, sess), X_test, Y_test)\n",
    "\n",
    "# notice: we do not need to use the session environment anymore, so returning from it.\n",
    "plt.title(\"Trained Classifier\")\n",
    "\n",
    "epoch = np.arange(len(train_cost))\n",
    "plt.figure()\n",
    "plt.plot(epoch,train_cost,'r',epoch,val_cost,'b')\n",
    "plt.legend(['Train Loss','Val Loss'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    " 1) A linear logistic classifier is only able to create a linear decision boundary. Change the Logistic classifier into a (non-linear) Neural network by inserting a dense hidden layer between the input and output layers of the model\n",
    " \n",
    " 2) Experiment with multiple hidden layers or more / less hidden units. What happens to the decision boundary?\n",
    " \n",
    " 3) Overfitting: When increasing the number of hidden layers / units the neural network will fit the training data better by creating a highly nonlinear decision boundary. If the model is to complex it will often generalize poorly to new data (validation and test set). Can you obseve this from the training and validation errors? \n",
    " \n",
    " 4) We used the vanilla stocastic gradient descent algorithm for parameter updates. This is usually slow to converge and more sophisticated pseudo-second-order methods usually works better. Try changing the optimizer to [adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) or [Momentum](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
